{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torchmetrics\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import wandb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if not all(\n",
    "    os.path.isfile(path) for path in [\"data/train.txt\", \"data/val.txt\", \"test.txt\"]\n",
    "):\n",
    "    # data paths\n",
    "    train_path = \"data/Train_Dst_NoAuction_DecPre_CF_7.txt\"\n",
    "    test_paths = [\n",
    "        \"data/Test_Dst_NoAuction_DecPre_CF_7.txt\",\n",
    "        \"data/Test_Dst_NoAuction_DecPre_CF_8.txt\",\n",
    "        \"data/Test_Dst_NoAuction_DecPre_CF_9.txt\",\n",
    "    ]\n",
    "\n",
    "    # download data\n",
    "    if not os.path.isfile(\"data/data.zip\"):\n",
    "        !wget \"https://raw.githubusercontent.com/zcakhaa/DeepLOB-Deep-Convolutional-Neural-Networks-for-Limit-Order-Books/master/data/data.zip\" -P data/\n",
    "        !unzip -n data/data.zip -d data/\n",
    "\n",
    "    # load training + validation data\n",
    "    train_val_data = np.loadtxt(train_path, unpack=True)\n",
    "\n",
    "    # split into train and validation\n",
    "    train_slice = slice(0, int(0.8 * train_val_data.shape[0]))\n",
    "    val_slice = slice(int(0.8 * train_val_data.shape[0]), train_val_data.shape[0])\n",
    "\n",
    "    train_data = train_val_data[train_slice, :]\n",
    "    val_data = train_val_data[val_slice, :]\n",
    "\n",
    "    # load test data\n",
    "    test_data = np.concatenate([np.loadtxt(path, unpack=True) for path in test_paths])\n",
    "\n",
    "    # save train, val, test data to single\n",
    "    np.savetxt(\"data/train.txt\", train_data.T)\n",
    "    np.savetxt(\"data/val.txt\", val_data.T)\n",
    "    np.savetxt(\"data/test.txt\", test_data.T)\n",
    "\n",
    "else:\n",
    "    # data paths\n",
    "    train_path = \"data/train.txt\"\n",
    "    val_path = \"data/val.txt\"\n",
    "    test_path = \"data/test.txt\"\n",
    "\n",
    "    # load train, val, test data\n",
    "    train_data = np.loadtxt(train_path)\n",
    "    val_data = np.loadtxt(val_path)\n",
    "    test_data = np.loadtxt(test_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "((203800, 149), (50950, 149), (139587, 149))"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class LobDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        window_length: int = 100,\n",
    "        prediction_horizon_index: int = 4,\n",
    "    ) -> None:\n",
    "        super(LobDataset, self).__init__()\n",
    "        data_copy = data.copy()\n",
    "        # As input, we select the first 40 columns. These are the first 10 levels\n",
    "        # of the orderbook, containing price and volume for both bid and ask\n",
    "        input_data = data_copy[:, :40]\n",
    "        # As labels, we select the last 5 columns of the orderbook.\n",
    "        # The labels are (1, 2, 3), which respectively represent\n",
    "        # (positive percentage change, stationary, negative percentage change).\n",
    "        labels = data_copy[:, -5:]\n",
    "        # Make the labels start from 0\n",
    "        labels -= 1\n",
    "\n",
    "        # Each of the 5 column of the labels represents\n",
    "        # a different prediction horizon (i.e., 1, 2, 3, 5, 10).\n",
    "        # We keep just one of those\n",
    "        labels = labels[:, prediction_horizon_index]\n",
    "\n",
    "        # Split the input data in windows of length `window_length`,\n",
    "        # and trim the first `window_length` elements of the labels\n",
    "        input_windows, labels_trimmed = self.sliding_window_data(\n",
    "            input_data, labels, window_length\n",
    "        )\n",
    "\n",
    "        # Cast np arrays into tensors and add one dimension to input to account for convolutions\n",
    "        self.input_windows = torch.tensor(input_windows, dtype=torch.float).unsqueeze(1)\n",
    "        self.labels = torch.tensor(labels_trimmed, dtype=torch.long)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.input_windows.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        return {\"input\": self.input_windows[idx], \"labels\": self.labels[idx]}\n",
    "\n",
    "    @staticmethod\n",
    "    def sliding_window_data(\n",
    "        input_data: np.ndarray, labels: np.ndarray, window_length: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        input_data = np.array(input_data)\n",
    "        labels = np.array(labels)\n",
    "        input_windows = sliding_window_view(\n",
    "            input_data, window_length, axis=0\n",
    "        ).transpose((0, 2, 1))\n",
    "        labels_trimmed = labels[window_length - 1 :]\n",
    "        return input_windows, labels_trimmed"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class ConvolutionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, activation_function: nn.Module\n",
    "    ) -> None:\n",
    "        super(ConvolutionBlock, self).__init__()\n",
    "\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (1, 2), (1, 2)),\n",
    "            activation_function,\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation_function,\n",
    "            nn.Conv2d(out_channels, out_channels, (4, 1)),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            activation_function,\n",
    "            nn.Conv2d(out_channels, out_channels, (4, 1)),\n",
    "            activation_function,\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv_block(x)\n",
    "\n",
    "\n",
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int) -> None:\n",
    "        super(InceptionBlock, self).__init__()\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (1, 1), padding=\"same\"),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, (3, 1), padding=\"same\"),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, (1, 1), padding=\"same\"),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, (5, 1), padding=\"same\"),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.MaxPool2d((3, 1), stride=(1, 1), padding=(1, 0)),\n",
    "            nn.Conv2d(in_channels, out_channels, (1, 1), padding=\"same\"),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out_1 = self.block1(x)\n",
    "        out_2 = self.block2(x)\n",
    "        out_3 = self.block3(x)\n",
    "\n",
    "        return torch.cat((out_1, out_2, out_3), dim=1)\n",
    "\n",
    "\n",
    "class DeepLOB(nn.Module):\n",
    "    def __init__(self, num_classes: int = 3) -> None:\n",
    "        super(DeepLOB, self).__init__()\n",
    "\n",
    "        # convolution blocks\n",
    "        self.conv_block1 = ConvolutionBlock(1, 32, activation_function=nn.LeakyReLU())\n",
    "        self.conv_block2 = ConvolutionBlock(32, 32, activation_function=nn.Tanh())\n",
    "\n",
    "        # convolution block 3 is not standard\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, (1, 10)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv2d(32, 32, (4, 1)),\n",
    "            nn.LeakyReLU(0.01),\n",
    "        )\n",
    "\n",
    "        # inception block\n",
    "        self.inception_block = InceptionBlock(32, 64)\n",
    "\n",
    "        # lstm layer + linear to output logits\n",
    "        self.lstm = nn.LSTM(input_size=192, hidden_size=64, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # conv blocks\n",
    "        out = self.conv_block1(x)\n",
    "        out = self.conv_block2(out)\n",
    "        out = self.conv_block3(out)\n",
    "\n",
    "        # inception block\n",
    "        out = self.inception_block(out)\n",
    "\n",
    "        # reshape data to feed lstm\n",
    "        out = out.permute(0, 2, 1, 3)\n",
    "        out = out.reshape(out.shape[0], -1, out.shape[2])\n",
    "\n",
    "        # use lstm and take last item\n",
    "        out, _ = self.lstm(out)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # return softmaxed classes\n",
    "        return torch.softmax(self.fc1(out), dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch Lightning modules"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class LobDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data: np.ndarray,\n",
    "        val_data: np.ndarray,\n",
    "        test_data: np.ndarray,\n",
    "        batch_size: int,\n",
    "    ) -> None:\n",
    "        super(LobDataModule, self).__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        if stage in [None, \"fit\"]:\n",
    "            self.trainset = LobDataset(self.train_data)\n",
    "            self.devset = LobDataset(self.val_data)\n",
    "        if stage in [None, \"test\"]:\n",
    "            self.testset = LobDataset(self.test_data)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.devset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.testset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class DeepLobModel(pl.LightningModule):\n",
    "    def __init__(self, hparams: Dict[str, Any]) -> None:\n",
    "        super(DeepLobModel, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        # model and criterion\n",
    "        self.model = DeepLOB(self.hparams.num_classes)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # metrics to track\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.train_f1 = torchmetrics.F1()\n",
    "        self.val_f1 = torchmetrics.F1()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "        self.test_f1 = torchmetrics.F1()\n",
    "\n",
    "    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(batch[\"input\"])\n",
    "\n",
    "    def step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        labels = batch[\"labels\"].view(-1)\n",
    "        logits = self(batch)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "\n",
    "        return {\"loss\": loss, \"predictions\": predictions}\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Dict[str, torch.Tensor], batch_idx: Optional[int]\n",
    "    ) -> torch.Tensor:\n",
    "        step_output = self.step(batch)\n",
    "        accuracy = self.train_acc(step_output[\"predictions\"], batch[\"labels\"])\n",
    "        f1_score = self.train_f1(step_output[\"predictions\"], batch[\"labels\"])\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": step_output[\"loss\"],\n",
    "                \"train_acc\": accuracy,\n",
    "                \"train_f1\": f1_score,\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "        return step_output[\"loss\"]\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Dict[str, torch.Tensor], batch_idx: Optional[int]\n",
    "    ) -> None:\n",
    "        step_output = self.step(batch)\n",
    "        accuracy = self.val_acc(step_output[\"predictions\"], batch[\"labels\"])\n",
    "        f1_score = self.val_f1(step_output[\"predictions\"], batch[\"labels\"])\n",
    "\n",
    "        self.log_dict(\n",
    "            {\"val_loss\": step_output[\"loss\"], \"val_acc\": accuracy, \"val_f1\": f1_score},\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Dict[str, torch.Tensor], batch_idx: Optional[int]\n",
    "    ) -> None:\n",
    "        step_output = self.step(batch)\n",
    "        accuracy = self.val_acc(step_output[\"predictions\"], batch[\"labels\"])\n",
    "        f1_score = self.val_f1(step_output[\"predictions\"], batch[\"labels\"])\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"test_loss\": step_output[\"loss\"],\n",
    "                \"test_acc\": accuracy,\n",
    "                \"test_f1\": f1_score,\n",
    "            },\n",
    "            prog_bar=True,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "\n",
    "hparams = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"num_classes\": 3,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model = DeepLobModel(hparams)\n",
    "\n",
    "datamodule = LobDataModule(train_data, val_data, test_data, hparams[\"batch_size\"])\n",
    "\n",
    "wandb_logger = WandbLogger(offline=False, project=\"DeepLOB-ai4t\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_acc\",\n",
    "    patience=20,\n",
    "    verbose=False,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./saved_checkpoints\",\n",
    "    filename=\"{epoch}_{val_acc:.3f}\",\n",
    "    monitor=\"val_acc\",\n",
    "    save_top_k=2,\n",
    "    save_last=True,\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=150,\n",
    "    num_sanity_val_steps=0,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "trainer.fit(model=model, datamodule=datamodule)\n",
    "\n",
    "wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.test(ckpt_path=\"best\", datamodule=datamodule)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}